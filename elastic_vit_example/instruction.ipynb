{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamil\\anaconda3\\envs\\vit_sampling\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import models_v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpatch_sampler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Flowers102\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolor_histograms_entropy_sampler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ColorHistogramsEntropySampler\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01medge_detector_limited_points_sampler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EdgeDetectorLimitedPointsSampler\n",
      "File \u001B[1;32m~\\OneDrive\\Pulpit\\przedmioty\\semestr 7\\pattern recognition\\projekt\\vit_sampling_heuristics\\src\\color_histograms_entropy_sampler.py:6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01melastic_vit_example\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpatch_sampler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GridSamplerV2, PatchSampler\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01melastic_vit_example\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcustom_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CustomDataset\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m batch_histogram\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample_patches\u001B[39m(image, n_patches_in_stages: \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     10\u001B[0m     grid_sampler \u001B[38;5;241m=\u001B[39m GridSamplerV2()\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from patch_sampler import *\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "from src.color_histograms_entropy_sampler import ColorHistogramsEntropySampler\n",
    "from src.edge_detector_limited_points_sampler import EdgeDetectorLimitedPointsSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, base_dataset, split, sampler):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sampler = sampler\n",
    "        self.split = split\n",
    "        self.augmentations = self._get_augmentations()\n",
    "    \n",
    "    def _get_augmentations(self):\n",
    "        train_transforms = [\n",
    "            transforms.Resize((240, 240)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(degrees=(-45, 45)),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.5),\n",
    "            transforms.Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2621, 0.2133, 0.2248])\n",
    "        ]\n",
    "        test_transforms = [\n",
    "            transforms.Resize((240, 240)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2621, 0.2133, 0.2248])\n",
    "        ]\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return transforms.Compose(train_transforms)\n",
    "        else:\n",
    "            return transforms.Compose(test_transforms)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.base_dataset[idx]\n",
    "        image = self.augmentations(image)\n",
    "        x, coords = self.sampler(image)\n",
    "        return x, coords, label, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomDataset jest klasą przetwarzającą dataset np. Flowers102 na patche i koordynaty w formacie przyjmowanym przez ElsaticVit.\n",
    "Tzn. w momencie wywołania metody getitem(index) z oryginalnego datasetu pobierany jest obraz o danym indeksie, nakładane są na niego augmentacje (w zależności od tego, czy jest to zbiór treningowy czy nie - dane argumentem 'split'), a następnie obraz dzielony jest na patche zgodnie z metodą daną argumentem 'sampler'.\n",
    "Normalizacja zbiorów powinna być zależna od średniej i odchylenia standardowego dla danego datasetu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset ten jest dostosowany do możliwości wykorzystania metody uczenia przez destylację, dlatego zwraca również obraz jako całość w postaci argumentu 'image'. Z powodu braku wykorzystania destylacji w aktualnym sposobie treningu, element ten możnaby pominąć, jednak należałoby wtedy wziąć to pod uwagę również w pętli treningowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej, przykłady użycia - GridSamplerV2 z domyślną liczbą patchy (14,14) dzieli obraz na standardowy grid. W ramach przykładu wykorzystano dataset Flowers102.\n",
    "W miejsce GridSamplerV2 należy podstawić dowolną opracowaną przez nas metodę podziału na patche, tak długo jak patche mają rozmiar 16x16, a obraz początkowy ma rozmiar 224x224. Koordynaty muszą mieścić się w tym zakresie i być zgodne z opisanymi w pracy, a więc przentować lewy górny i prawy dolny róg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler = GridSamplerV2(patches_num_yx=(14,14))\n",
    "sampler =\n",
    "base_train_dataset = Flowers102('Flowers102', split='train',download=True)\n",
    "base_val_dataset = Flowers102('Flowers102', split='val',download=True)\n",
    "flowers_train_dataset = CustomDataset(base_train_dataset, 'train', sampler)\n",
    "flowers_valid_dataset = CustomDataset(base_val_dataset, 'val', sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardowe dataloadery dla zbioru treningowego i walidacyjnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Set an appropriate batch size\n",
    "train_loader = DataLoader(flowers_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(flowers_valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytujemy model oparty na standardowym modelu deit jednak przyjmującym patche i koordynaty zamiast zdjęć. Wczytujemy również zapamiętany state uczenia zawierający przede wszystkim wagi wytrenowanego modelu, znajdujące się w słowniku pod nazwą model (dla mnie najlepiej zadziałał chceckpoint 'elastic-224-70random30grid.pth'). W systemie Windows konieczne może być przetworzenie ścieżki do pliku jak niżej, w przeciwnym wypadku powoduje błąd wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_v2.deit_base_patch16_LS()\n",
    "state = torch.load('elastic-224-70random30grid.pth')\n",
    "loaded_model = state['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba klas w pretrenowanym modelu (1000) najprawdopodobniej nie zgadza się z liczbą klas w użytym datasecie. W przykładowym przypadku liczba klas wynosi 102, więc trzeba zresetować ostatnią warstwę - klasyfikator. Model posiada taką metodę wbudowaną, przyjmujacą liczbę klas jako parametr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_classifier(num_classes=102) #for flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przenosimy model na GPU (CUDA) jeśli jest taka możliwość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tunowanie modelu powinno wykorzystać pretrenowane wagi. W tym celu zalecam rozpoczęcie treningu od trenowania wyłącznie nowopowstałej warstwy klasyfikacyjnej (head). W tym celu ustawiamy liczenie gradientów wyłącznie dla tej warstwy, wyłączając je dla pozostałych warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.head.weight.requires_grad = True\n",
    "model.head.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ warstwa klasyfikacyjna jest pusta, wymaga większej liczby epok uczenia, jak również może korzystać z wyższego learning rate'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardowa pętla uczenia wraz z walidacją po każdej epoce. Wykorzystano tqdm w celu lepszej prezentacji postępu uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_outputs = 0\n",
    "    for i, data in enumerate(tqdm(train_loader), 0):\n",
    "        x, coords, labels, images = data\n",
    "        x, coords, labels, images = x.to(device), coords.to(device), labels.to(device), images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, coords)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_correct += (torch.argmax(outputs, dim=-1) == labels).sum().item()\n",
    "        train_outputs += outputs.shape[0]\n",
    "\n",
    "        running_loss += loss\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_outputs = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(valid_loader), 0):\n",
    "            x, coords, labels,_ = data\n",
    "            x, coords, labels = x.to(device), coords.to(device), labels.to(device)\n",
    "            outputs = model(x, coords)\n",
    "            correct = (torch.argmax(outputs, dim=-1) == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_outputs += outputs.shape[0]\n",
    "    print(f\"[Epoch {epoch + 1}] Loss: {running_loss / i:.3f}, Train Acc: {train_correct/train_outputs:.3f}, Valid Acc: {total_correct/total_outputs:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poprawiony model można zapisać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"85-12.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A następnie wczytać do dalszych testów po wcześniejszym zdefiniowaniu tak samo jak wcześniej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"85-12.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu poprawienia accuracy, zalecam dodatkowo dotrenować warstwy powiązane z patchami i pozycjami, ale dopiero po wytrenowaniu klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.head.weight.requires_grad = True\n",
    "model.head.bias.requires_grad = True\n",
    "model.pos_embed.requires_grad = True\n",
    "model.patch_embed.proj.weight.requires_grad = True\n",
    "model.patch_embed.proj.bias.requires_grad = True\n",
    "for p in model.patch_embed.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając dotrenowane te parametry można ponowić trening odblokowując warstwy atencji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_p, p in model.named_parameters():\n",
    "    if '.attn.' in name_p:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "model.head.weight.requires_grad = True\n",
    "model.head.bias.requires_grad = True\n",
    "model.pos_embed.requires_grad = True\n",
    "model.patch_embed.proj.weight.requires_grad = True\n",
    "model.patch_embed.proj.bias.requires_grad = True\n",
    "for p in model.patch_embed.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stosując te trzy stopnie fine tuningu udało mi się uzyskać accuracy na zbiorze walidacyjnym na poziomie 85% i ponad 84% na zbiorze testowym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo model ma pewne parametry, między innymi trzy rodzaje dropoutu, które można ustawić w celu zmniejszenia overfittingu. Wtedy część wag, atencji lub patchy jest losow ignorowana podczas uczenia. Trzeba z tym jednak uważać, szczególnie jeśli chodzi o drop_path_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_v2.deit_base_patch16_LS(drop_rate=0.25, attn_drop_rate=0.25, drop_path_rate=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deit jest szczególnym przypadkiem ViT, który umożliwia trenowanie z użyciem destylacji. Oznacza to, że można wykorzystać inną wytrenowaną sieć. Pozwala to wykorzystać nie tylko klasyfikację, ale również poszczególne prawdopodobieństwa zwracane przez sieć, co w pewnych sytuacjach może ułatwiać uczenie. W przypadku pretrenowanego Elastic-ViT dla datasetu Flowers102 nie zauważyłem szczególnej poprawy dokładności, a konieczność zarówno wytrenowania drugiego modelu, jak również konieczność ewaluacji wyników na dwóch modelach znacząco wydłuża czas treningu. Na ten moment nie widzę sensu zaciemniania obrazu poprzez dodanie tego kroku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
